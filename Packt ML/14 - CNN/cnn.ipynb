{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    "\n",
    "### Understanding CNNs\n",
    "Certain types of NNs, particularly CNNs, can automatically learn features from raw datasets that are most useful for the task at hand.\n",
    "\n",
    "It is common to consider early layers of CNN as feature extractors while later layers are usually fully connected (i.e. a MLP) to use the extracted features from the CNN to perform a regression of classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions for Classification\n",
    "\n",
    "**Binary Cross Entropy** is the loss function for binary classification while **Categorical Cross-Entropy** is the loss function for multiclass classification.\n",
    "\n",
    "For Binary Classification: \n",
    "1. `BCELoss` : pass in class probabilities\n",
    "2. `BCEWithLogitsLoss` : pass in the logits\n",
    "\n",
    "For Multiclass Classification:\n",
    "1. `NLLLoss` (Negative Log Likelihood) : pass in log probabilites\n",
    "2. `CrossEntropyLoss` : preferred that logits are passed in due to numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import platform as pl\n",
    "\n",
    "use_gpu = True\n",
    "\n",
    "if pl.system().lower() == \"linux\" and use_gpu:\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "elif pl.system().lower() == \"linux\" and not use_gpu:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Cross-entropy\n",
    "logits = torch.Tensor([0.8])\n",
    "target = torch.Tensor([1.0])\n",
    "probas = torch.sigmoid(logits)\n",
    "bce_loss_fn = nn.BCELoss()\n",
    "bce_logits_loss_fn = nn.BCEWithLogitsLoss()\n",
    "print(f\"BCE (w/probas) = {bce_loss_fn(probas, target):.4f}\")\n",
    "print(f\"BCE (w/logits) = {bce_logits_loss_fn(logits, target):.4f}\")\n",
    "\n",
    "# Categorical Cross-entropy\n",
    "logits = torch.Tensor([[1.5, 0.8, 2.1]])\n",
    "target = torch.Tensor([2]).type(torch.LongTensor) # torch will not accept a float tensor as categorical target\n",
    "                                                  # need to cast type Long\n",
    "probas = torch.log(torch.softmax(logits, dim=1))\n",
    "cce_loss_fn = nn.NLLLoss()\n",
    "cce_loss_logits_fn = nn.CrossEntropyLoss()\n",
    "print(f\"CCE (w/probas) = {cce_loss_fn(probas, target):.4f}\")\n",
    "print(f\"CCE (w/logits) = {cce_loss_logits_fn(logits, target):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./\"\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_dataset = torchvision.datasets.MNIST(root=image_path, train=True, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "mnist_val = Subset(mnist_dataset, torch.arange(10000))\n",
    "mnist_train = Subset(mnist_dataset, torch.arange(10000, len(mnist_dataset)))\n",
    "mnist_test = torchvision.datasets.MNIST(root=image_path, train=False, transform=transform, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 64\n",
    "torch.manual_seed(42)\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that PyTorch expects image batchs in *NCHW* format (num_batch_img x num_channel x height x width)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, add_flatten=False):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        # convolution layers\n",
    "        layers.append(nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5, padding=2))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.MaxPool2d(kernel_size=2))\n",
    "        layers.append(nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=2))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.MaxPool2d(kernel_size=2))\n",
    "\n",
    "        if add_flatten:\n",
    "            layers.append(nn.Flatten())\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def add_fully_connected(self, input_size, output_size, hidden_sizes):\n",
    "        assert type(hidden_sizes) == list, \"Hidden Sizes not a list.\"  \n",
    "        self.layers.add_module(\"FC1\", nn.Linear(input_size, hidden_sizes[0]))\n",
    "        self.layers.add_module(\"ReLU\", nn.ReLU())\n",
    "        self.layers.add_module(\"Dropout\", nn.Dropout(p=0.5))\n",
    "        self.layers.add_module(\"FC2\", nn.Linear(hidden_sizes[-1], output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate size of the feature maps\n",
    "model = CNN(add_flatten=False)\n",
    "x = torch.ones([4, 1, 28, 28])\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now add fully connected layers\n",
    "model = CNN(add_flatten=True)\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_fully_connected(input_size=3136, output_size=10, hidden_sizes=[1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, loss_fn, optimizer, num_epochs=20, log_idx=1, verbose=False):\n",
    "    model.to(device)\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for e in range(num_epochs):\n",
    "        # train\n",
    "        train_acc, train_examples, train_loss = 0.0, 0, 0.0\n",
    "        model.train()\n",
    "        for features, labels in train_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "            logits = model(features)\n",
    "            probas = torch.sigmoid(logits)\n",
    "            preds  = torch.argmax(probas, dim=1)\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            train_acc += (preds == labels).sum().item()\n",
    "            train_loss += loss.item()\n",
    "            train_examples += features.shape[0]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_losses.append(train_loss/train_examples)\n",
    "        train_accuracies.append(train_acc/train_examples)\n",
    "\n",
    "        # validation\n",
    "        val_acc, val_examples = 0.0, 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader:\n",
    "                features, labels = features.to(device), labels.to(device)\n",
    "                logits = model(features)\n",
    "                probas = torch.sigmoid(logits)\n",
    "                preds  = torch.argmax(probas, dim=1)\n",
    "                val_acc += (preds == labels).sum().item()\n",
    "                val_examples += features.shape[0]\n",
    "\n",
    "            val_accuracies.append(val_acc/val_examples)\n",
    "\n",
    "        if verbose and e % log_idx == 0:\n",
    "            print(f\"Epoch {e}/{num_epochs}: Train Loss = {train_loss/train_examples:.4f} | \"\n",
    "                  f\"Train Acc = {train_acc/train_examples:.4f} | \"  \n",
    "                  f\"Val Acc = {val_acc/val_examples}\")\n",
    "\n",
    "    return train_losses, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_accuracies, val_accuracies = train(model, train_loader, val_loader, loss_fn, optimizer, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c15b0dd616b9d947ea13c87fa269e05e8054758c0f7c261fec597f1be7e65f2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('hands_on_ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
